from sentence_transformers import SentenceTransformer, util
import numpy as np
import math
from collections import Counter

# Load model once for efficiency
model_cosine = SentenceTransformer('all-MiniLM-L6-v2')

# Function to calculate cosine similarity
def calculate_cosine_similarity(reference, response):
    # Generate embeddings for reference and response
    ref_embedding = model_cosine.encode(reference, convert_to_tensor=True)
    resp_embedding = model_cosine.encode(response, convert_to_tensor=True)
    # Calculate cosine similarity
    similarity_score = util.cos_sim(ref_embedding, resp_embedding).item()
    return similarity_score

# Initialize a pre-trained model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def get_closest_reference(user_question, reference_answers):
    # Embed the user question and all reference answers
    question_embedding = model.encode([user_question])
    reference_embeddings = model.encode(list(reference_answers.keys()))
    
    # Compute cosine similarity between the user question and all reference answers
    similarities = np.dot(question_embedding, reference_embeddings.T)
    
    # Find the index of the most similar reference
    most_similar_idx = np.argmax(similarities)
    
    # Return the most similar reference
    return list(reference_answers.values())[most_similar_idx] if similarities[0][most_similar_idx] > 0 else None


# Function to calculate BLEU score
import math
from collections import Counter
from itertools import chain

def calculate_bleu(reference, candidate, max_n=2):
    """
    Calculates BLEU score from scratch.
    :param reference: The reference text (string).
    :param candidate: The candidate text generated by the model (string).
    :param max_n: Maximum n-gram order to consider (default is bigram, 2).
    :return: BLEU score as a float.
    """

    # Step 1: Tokenize reference and candidate texts
    reference_tokens = reference.split()
    candidate_tokens = candidate.split()

    # Flatten tokens if necessary
    reference_tokens = list(chain.from_iterable(reference_tokens)) if any(isinstance(t, list) for t in reference_tokens) else reference_tokens
    candidate_tokens = list(chain.from_iterable(candidate_tokens)) if any(isinstance(t, list) for t in candidate_tokens) else candidate_tokens

    # Step 2: Calculate precision for each n-gram order
    precisions = []
    for n in range(1, max_n + 1):
        # Generate n-grams
        ref_ngrams = Counter([tuple(reference_tokens[i:i + n]) for i in range(len(reference_tokens) - n + 1)])
        cand_ngrams = Counter([tuple(candidate_tokens[i:i + n]) for i in range(len(candidate_tokens) - n + 1)])

        # Count matches
        match_count = sum(min(count, cand_ngrams[ngram]) for ngram, count in ref_ngrams.items())
        total_count = sum(cand_ngrams.values())

        # Precision for this n-gram level
        precision = match_count / total_count if total_count > 0 else 0
        precisions.append(precision)

    # Step 3: Calculate geometric mean of n-gram precisions
    if all(p > 0 for p in precisions):
        geometric_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)
    else:
        geometric_mean = 0

    # Step 4: Calculate brevity penalty
    ref_length = len(reference_tokens)
    cand_length = len(candidate_tokens)

    if cand_length > ref_length:
        brevity_penalty = 1
    else:
        brevity_penalty = math.exp(1 - ref_length / cand_length) if cand_length > 0 else 0

    # Step 5: Combine geometric mean and brevity penalty to get BLEU score
    bleu_score = brevity_penalty * geometric_mean

    return bleu_score



def calculate_rouge_n(reference, response, n=1):
    """
    Calculate ROUGE-N (precision, recall, F1-score).
    :param reference: Reference text (string).
    :param response: Model-generated text (string).
    :param n: N-gram size (default is 1, unigram).
    :return: Precision, Recall, and F1-score.
    """
    # Tokenize reference and response
    reference_tokens = reference.split()
    response_tokens = response.split()

    # Generate n-grams
    ref_ngrams = Counter(tuple(reference_tokens[i:i + n]) for i in range(len(reference_tokens) - n + 1))
    resp_ngrams = Counter(tuple(response_tokens[i:i + n]) for i in range(len(response_tokens) - n + 1))

    # Calculate matches
    overlap = sum(min(ref_ngrams[ngram], resp_ngrams[ngram]) for ngram in ref_ngrams)
    ref_total = sum(ref_ngrams.values())
    resp_total = sum(resp_ngrams.values())

    # Precision, Recall, F1
    precision = overlap / resp_total if resp_total > 0 else 0
    recall = overlap / ref_total if ref_total > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0

    return precision, recall, f1


def calculate_rouge_l(reference, response):
    """
    Calculate ROUGE-L (Longest Common Subsequence).
    :param reference: Reference text (string).
    :param response: Model-generated text (string).
    :return: Precision, Recall, and F1-score.
    """
    # Tokenize reference and response
    reference_tokens = reference.split()
    response_tokens = response.split()

    # Helper function to compute LCS length
    def lcs_length(x, y):
        dp = [[0] * (len(y) + 1) for _ in range(len(x) + 1)]
        for i in range(1, len(x) + 1):
            for j in range(1, len(y) + 1):
                if x[i - 1] == y[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                else:
                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
        return dp[-1][-1]

    lcs_len = lcs_length(reference_tokens, response_tokens)

    # Precision, Recall, F1
    precision = lcs_len / len(response_tokens) if len(response_tokens) > 0 else 0
    recall = lcs_len / len(reference_tokens) if len(reference_tokens) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0

    return precision, recall, f1